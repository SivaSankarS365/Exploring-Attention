{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch numpy scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention: Bits, use it inspect and visualize line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first setup a small dummy input for testing and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "S = 8\n",
    "E = 16\n",
    "\n",
    "z = torch.rand(B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct query, key and value from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_proj = nn.Linear(E,E)\n",
    "key_proj = nn.Linear(E,E)\n",
    "value_proj = nn.Linear(E,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_proj(z)\n",
    "key = key_proj(z)\n",
    "value = value_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 16]), torch.Size([2, 8, 16]), torch.Size([2, 8, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_t = key.transpose(1,2)\n",
    "key_t.shape # Q.K' : [B,S,E] @ [B,E,S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_attention_logits = (query@key_t) # Q.K' : [B,S,E] @ [B,E,S] => [B,S,S]\n",
    "unscaled_attention_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale our attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,E = query.shape\n",
    "scale = math.sqrt(E)\n",
    "attention_logits = unscaled_attention_logits/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.inf\n",
    "causal_mask = torch.tril(torch.ones(B,S,S)) # lower trianglular matrix with 1s\n",
    "masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) # replace upper triangular with -inf or a very large negative number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1849,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.1462, -0.1943,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.2337, -0.2623, -0.1436,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.1217, -0.2063, -0.1253, -0.1600,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.2344, -0.2627, -0.3572, -0.2270, -0.1346,    -inf,    -inf,    -inf],\n",
       "        [-0.1876, -0.1997, -0.1351, -0.1944, -0.1529, -0.1729,    -inf,    -inf],\n",
       "        [-0.2344, -0.2535, -0.2790, -0.2311, -0.1611, -0.2323, -0.1757,    -inf],\n",
       "        [-0.2373, -0.2316, -0.2593, -0.1755, -0.1306, -0.1911, -0.1500, -0.1635]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attention_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attention_weights = torch.softmax(masked_attention_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5120, 0.4880, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3261, 0.3170, 0.3569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2579, 0.2370, 0.2570, 0.2482, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2013, 0.1956, 0.1780, 0.2027, 0.2224, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1643, 0.1624, 0.1732, 0.1632, 0.1701, 0.1668, 0.0000, 0.0000],\n",
       "         [0.1413, 0.1386, 0.1351, 0.1417, 0.1520, 0.1416, 0.1498, 0.0000],\n",
       "         [0.1194, 0.1201, 0.1168, 0.1270, 0.1328, 0.1250, 0.1303, 0.1285]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5205, 0.4795, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3454, 0.3293, 0.3253, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2427, 0.2459, 0.2474, 0.2640, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2016, 0.2007, 0.2048, 0.2024, 0.1905, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1580, 0.1618, 0.1743, 0.1769, 0.1579, 0.1710, 0.0000, 0.0000],\n",
       "         [0.1412, 0.1420, 0.1418, 0.1382, 0.1416, 0.1526, 0.1426, 0.0000],\n",
       "         [0.1209, 0.1197, 0.1240, 0.1306, 0.1165, 0.1244, 0.1266, 0.1374]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_weights # Observe lower triangular! Causal attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = causal_attention_weights@value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about padding tokens?\n",
    "* In the base implementation we used self-attention on every-tokens, including padding tokens if present!\n",
    "* Now we extend the base implementation so that attention is ignored for tokens like pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at a sample tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # Important!,usually decoder only models require left padding\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id # pad token id for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,     2,     2,     2,     2,     2,     1, 13264,  5509])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'][0] # Observe left padding!, 2 is the pad token\n",
    "# Refer: https://huggingface.co/docs/transformers/llm_tutorial#wrong-padding-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['attention_mask'][0] # Observe attention mask is zero for pad tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle attention mask in the attention logits\n",
    "Also look at https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  3.8209e-01,  4.0718e-01,  1.3385e-02],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  1.8255e-01,  3.2008e-01,  3.2200e-02],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  8.1127e-01,  4.1283e-02,  8.8291e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  7.6152e-01,  9.2479e-01,  8.3445e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  5.4773e-01,  2.6998e-01,  5.0050e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  4.7296e-01,  4.2571e-01,  8.8768e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  7.3142e-01,  3.2486e-01,  4.0470e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  5.1995e-03,  1.0472e-01,  3.8860e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  1.4755e-01,  5.9521e-01,  4.8296e-01]],\n",
       "\n",
       "        [[-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  5.7116e-01,\n",
       "           6.3688e-01,  8.8677e-01,  5.5978e-02,  6.9939e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  3.8668e-01,\n",
       "           3.1765e-01,  5.7342e-02,  1.1707e-01,  5.8055e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  7.2302e-01,\n",
       "           3.2769e-01,  3.4571e-02,  6.4907e-01,  5.9500e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  4.3871e-01,\n",
       "           5.9206e-01,  9.8321e-01,  9.2041e-01,  6.8307e-02],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  5.0815e-01,\n",
       "           7.3666e-01,  1.9559e-01,  3.9132e-01,  6.2323e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  1.6535e-01,\n",
       "           9.3228e-01,  2.2317e-01,  1.4979e-01,  2.4625e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  4.3133e-01,\n",
       "           9.4057e-01,  1.2264e-01,  2.9807e-01,  8.4143e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  5.3597e-01,\n",
       "           6.3612e-01,  8.4403e-01,  9.1977e-01,  9.6113e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  7.5814e-01,\n",
       "           6.9037e-01,  1.2216e-02,  5.8172e-02,  6.9283e-01]],\n",
       "\n",
       "        [[ 3.5623e-01,  4.2737e-01,  7.2605e-01,  4.8353e-01,  2.3791e-02,\n",
       "           5.1615e-01,  7.4232e-01,  2.3982e-01,  2.1001e-01],\n",
       "         [ 7.0362e-01,  7.6184e-01,  2.2505e-01,  4.6469e-01,  6.8316e-01,\n",
       "           3.8024e-01,  9.6772e-01,  2.3761e-01,  2.3100e-01],\n",
       "         [ 4.5068e-01,  8.3077e-01,  4.4372e-01,  1.8893e-01,  1.8273e-01,\n",
       "           3.0397e-01,  2.3073e-01,  7.3199e-01,  5.2987e-01],\n",
       "         [ 2.9439e-01,  4.6606e-01,  2.0205e-01,  4.1357e-01,  3.6179e-01,\n",
       "           3.1689e-01,  8.7118e-01,  7.5997e-03,  7.5876e-02],\n",
       "         [ 8.5640e-01,  5.5549e-01,  1.4704e-02,  7.2122e-01,  8.3457e-02,\n",
       "           6.3441e-01,  7.0479e-01,  7.8485e-01,  5.2333e-02],\n",
       "         [ 2.9362e-01,  6.5793e-01,  6.5687e-01,  1.5475e-01,  9.4438e-01,\n",
       "           5.8673e-02,  4.3231e-01,  3.7464e-01,  7.8254e-01],\n",
       "         [ 2.0147e-01,  4.4446e-01,  2.9531e-01,  9.7181e-01,  8.6654e-01,\n",
       "           8.9688e-01,  4.9426e-01,  5.3534e-01,  1.1481e-02],\n",
       "         [ 6.2146e-01,  9.7379e-01,  9.7437e-01,  6.1143e-01,  2.4873e-01,\n",
       "           5.2508e-01,  8.8730e-02,  7.9029e-01,  7.0751e-01],\n",
       "         [ 2.7793e-01,  4.3610e-01,  2.3815e-01,  8.3883e-01,  5.0684e-01,\n",
       "           7.8062e-01,  4.0799e-01,  8.2685e-01,  5.5504e-01]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_logits = torch.rand(B, S, S) # dummy for demo\n",
    "attention_logits = attention_logits.masked_fill_(attention_mask[:,None,:] == 0, torch.finfo(attention_logits.dtype).min)\n",
    "attention_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the code bits together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOutput:\n",
    "    def __init__(self,**kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "\n",
    "    Not Implemented:\n",
    "    * KV cacheing\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,S,S = attention_logits.shape\n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,S,S)).to(device)\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(attention_logits.dtype).min\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits,mask_value=None):\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(masked_attention_logits.dtype).min\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, mask_value)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 16\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output = self_atten(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-V caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith KV caching:\\nInput: (B,1,E), kv_cache\\nOutput: (B,1,E)\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\\\n",
    "\n",
    "With KV caching:\n",
    "Input: (B,1,E), kv_cache\n",
    "Output: (B,1,E)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Dummy input for testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "\n",
    "z_old = torch.rand(B,S,E)\n",
    "z = torch.rand(B,1,E)\n",
    "z_new = torch.concat([z_old,z],dim=1)\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output_old = self_atten(z_old,attention_mask=None)\n",
    "output_new = self_atten(z_new,attention_mask=None)\n",
    "\n",
    "print(z.shape)\n",
    "print(z_old.shape)\n",
    "print(z_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self,key,value,**kwargs):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store previous K,V in the cache\n",
    "cache = KVCache(output_old.key,\n",
    "                output_old.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct K,V from cache\n",
    "\n",
    "new_key = self_atten.key_proj(z)\n",
    "new_value = self_atten.value_proj(z)\n",
    "query = self_atten.query_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = torch.concat([cache.key,\n",
    "                    new_key],dim=1)\n",
    "value = torch.concat([cache.value,\n",
    "                      new_value],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [2, 1, 5] doesn't match the broadcast shape [2, 5, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m unmasked_attention_logits \u001b[38;5;241m=\u001b[39m self_atten\u001b[38;5;241m.\u001b[39mcalculate_unmasked_attention_logits(query,key)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Apply causal masking\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m masked_attention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mself_atten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43munmasked_attention_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate attention weights\u001b[39;00m\n\u001b[1;32m     10\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m self_atten\u001b[38;5;241m.\u001b[39mcalculate_attention_weights(masked_attention_logits)\n",
      "Cell \u001b[0;32mIn[24], line 49\u001b[0m, in \u001b[0;36mBaseSelfAttention.apply_causal_mask\u001b[0;34m(self, attention_logits, mask_value)\u001b[0m\n\u001b[1;32m     46\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attention_logits\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# replace upper triangular with -inf or a very large negative number, causal masking!\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m masked_attention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mattention_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m masked_attention_logits\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [2, 1, 5] doesn't match the broadcast shape [2, 5, 5]"
     ]
    }
   ],
   "source": [
    "# remaining operations remains the same\n",
    "\n",
    "# Calculate logits\n",
    "unmasked_attention_logits = self_atten.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "# Apply causal masking\n",
    "masked_attention_logits = self_atten.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "# Calculate attention weights\n",
    "attention_weights = self_atten.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "# And finally, Calculate the final output\n",
    "attention_output = self_atten.calculate_final_output(attention_weights,value)\n",
    "\n",
    "output_with_caching = BaseOutput(attention_output=attention_output,\n",
    "                    attention_weights=attention_weights,\n",
    "                    key=key,\n",
    "                    value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, causal mask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n",
      "torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# let's replicate it\n",
    "\n",
    "attention_logits = self_atten.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "B,Sq,Sk = attention_logits.shape # <---------Change 1\n",
    "\n",
    "device = attention_logits.device\n",
    "causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device) # <------------Change 2\n",
    "\n",
    "print(causal_mask.shape)\n",
    "causal_mask = causal_mask[:,-Sq:,-Sk:] # <------------Change 3\n",
    "print(causal_mask.shape)\n",
    "\n",
    "if mask_value is None:\n",
    "    mask_value = -torch.inf\n",
    "\n",
    "# replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue from that\n",
    "\n",
    "# Calculate attention weights\n",
    "attention_weights = self_atten.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "# And finally, Calculate the final output\n",
    "attention_output = self_atten.calculate_final_output(attention_weights,value)\n",
    "\n",
    "output_with_caching = BaseOutput(attention_output=attention_output,\n",
    "                    attention_weights=attention_weights,\n",
    "                    key=key,\n",
    "                    value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1127,  0.2380,  0.1639,  0.5565,  0.2931, -0.0349, -0.0777,\n",
       "          -0.0873,  0.1724, -0.1370,  0.4725,  0.0513,  0.0438,  0.1115,\n",
       "           0.0340, -0.1261]],\n",
       "\n",
       "        [[-0.0409,  0.2507,  0.0951,  0.5170,  0.3235, -0.0853, -0.1103,\n",
       "          -0.0243,  0.0784, -0.1308,  0.4732, -0.0428,  0.1069,  0.0872,\n",
       "          -0.0407, -0.2352]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1127,  0.2380,  0.1639,  0.5565,  0.2931, -0.0349, -0.0777,\n",
       "          -0.0873,  0.1724, -0.1370,  0.4725,  0.0513,  0.0438,  0.1115,\n",
       "           0.0340, -0.1261]],\n",
       "\n",
       "        [[-0.0409,  0.2507,  0.0951,  0.5170,  0.3235, -0.0853, -0.1103,\n",
       "          -0.0243,  0.0784, -0.1308,  0.4732, -0.0428,  0.1069,  0.0872,\n",
       "          -0.0407, -0.2352]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge it with the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x,kv_cache):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        if kv_cache is not None:\n",
    "            key = torch.concat([kv_cache.key,\n",
    "                    key],dim=1)\n",
    "            value = torch.concat([kv_cache.value,\n",
    "                      value],dim=1)\n",
    "\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,Sq,Sk = attention_logits.shape \n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device)\n",
    "        causal_mask = causal_mask[:,-Sq:,-Sk:] # Trim off, for kv_cache, no-op if kv_cache is None\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(attention_logits.dtype).min\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits,mask_value=None):\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(masked_attention_logits.dtype).min\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, mask_value)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None,kv_cache=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x,kv_cache=kv_cache)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self,key,value,**kwargs):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n",
    "\n",
    "\n",
    "class BaseOutput:\n",
    "    def __init__(self,**kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 16\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output = self_atten(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input for kv-cache testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "self_atten = BaseSelfAttention(E)\n",
    "\n",
    "output = self_atten(z,attention_mask=None)\n",
    "cache = KVCache(output.key,output.value) # Construct kv cache\n",
    "\n",
    "# new token\n",
    "_z = torch.rand(B,1,E)\n",
    "\n",
    "# Without KV Caching\n",
    "z_new = torch.concat([z,_z],dim=1)\n",
    "output_new = self_atten(z_new,attention_mask=None)\n",
    "\n",
    "# With KV Caching\n",
    "output_with_caching = self_atten(_z,attention_mask=None,kv_cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3471,  0.4393, -0.5275,  0.5012, -0.3518,  0.0159, -0.0625, -0.2379,\n",
       "          0.0965, -0.1188, -0.2035,  0.1183, -0.1810,  0.1572,  0.1148, -0.1510],\n",
       "        [ 0.4791,  0.4694, -0.5979,  0.5361, -0.4261,  0.0571, -0.0666, -0.2384,\n",
       "          0.0965, -0.0262, -0.1194,  0.1316, -0.3341,  0.1024,  0.1217, -0.3082]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3471,  0.4393, -0.5275,  0.5012, -0.3518,  0.0159, -0.0625,\n",
       "          -0.2379,  0.0965, -0.1188, -0.2035,  0.1183, -0.1810,  0.1572,\n",
       "           0.1148, -0.1510]],\n",
       "\n",
       "        [[ 0.4791,  0.4694, -0.5979,  0.5361, -0.4261,  0.0571, -0.0666,\n",
       "          -0.2384,  0.0965, -0.0262, -0.1194,  0.1316, -0.3341,  0.1024,\n",
       "           0.1217, -0.3082]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9802e-08,  0.0000e+00,  5.9605e-08,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -1.4901e-08,  1.4901e-08,  7.4506e-09,\n",
       "           0.0000e+00,  1.4901e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -1.4901e-08]],\n",
       "\n",
       "        [[ 0.0000e+00,  0.0000e+00,  5.9605e-08,  0.0000e+00,  2.9802e-08,\n",
       "           1.4901e-08,  0.0000e+00,  1.4901e-08,  2.9802e-08, -1.4901e-08,\n",
       "          -7.4506e-09,  0.0000e+00,  0.0000e+00, -2.9802e-08,  0.0000e+00,\n",
       "           0.0000e+00]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:] - output_with_caching.attention_output\n",
    "# Just to verify, some numerical errors are expected only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to multiple-heads | Multi-Headed Attention\n",
    "Functionality wise most of the code remains same. Hacky way to reuse self-attention code is by reshaping the q,k,v to shape `(batch*num_heads, S, E//num_heads)`. \n",
    "\n",
    "We will implement from scratch with seperate dimension for heads for better flexibility later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Q,K,V into heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "S = 8\n",
    "E = 32\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "self_attn = BaseSelfAttention(E)\n",
    "output = self_attn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the self-attention implementation\n",
    "\n",
    "query,key,value = self_attn.construct_query_key_value(z,kv_cache=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "\n",
    "\n",
    "B,S,E = query.shape\n",
    "assert E%num_heads==0, \"embed_dim must be divisible by num_heads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.view(B,num_heads,S,E//num_heads)\n",
    "key = key.view(B,num_heads,S,E//num_heads)\n",
    "value = value.view(B,num_heads,S,E//num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 8, 8])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape # now (B,num_heads,S,E//num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this, the operations are same as the original self-attention.\n",
    "# however,Some changes are required to incoperate an additional head dimension\n",
    "# After all operations, we merge back the heads before final output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = torch.rand(B,num_heads,S,E//num_heads) # dummy for testing\n",
    "\n",
    "attention_output = attention_output.view(B,S,E) # go back to (B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend self-attention to multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.arange(6*32).view(2,3,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,E = z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def _split_head(self,x,num_heads):\n",
    "        B,S,E = x.shape\n",
    "        assert E%num_heads==0, \"embed_dim must be divisible by num_heads\"\n",
    "        x = x.view(B,S,E//num_heads,num_heads) # B,S,E,H\n",
    "        x = x.permute([0,3,1,2]) # B,H, S, E\n",
    "        return x\n",
    "    \n",
    "    def _merge_head(self,x):\n",
    "        B,H,S,E = x.shape\n",
    "        x = x.permute([0,2,1,3]) # B,S,H,E\n",
    "        x = x.reshape(B,S,H*E) # B,S,H*E\n",
    "        return x\n",
    "\n",
    "\n",
    "    def create_heads(self,query,key,value):\n",
    "        B,S,E = query.shape\n",
    "        assert E%self.num_heads==0, \"embed_dim must be divisible by num_heads\"\n",
    "        # query = query.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        # key = key.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        # value = value.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        query = self._split_head(query,self.num_heads)\n",
    "        key = self._split_head(key,self.num_heads)\n",
    "        value = self._split_head(value,self.num_heads)\n",
    "        return query,key,value\n",
    "\n",
    "    def construct_query_key_value(self,x,kv_cache=None):\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "\n",
    "        query,key,value = self.create_heads(query,key,value)\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            key = torch.concat([kv_cache.key,\n",
    "                    key],dim=2) # B,H,S,E. dim becomes 2\n",
    "            value = torch.concat([kv_cache.value,\n",
    "                      value],dim=2)\n",
    "        \n",
    "        return query,key,value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,H,S,E] @ [B,H,E,S]\n",
    "        key_t = key.transpose(2,3) # Transpose to [B,H,E,S] by exchanging dim 2 and 3\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,H,Sq,Sk = attention_logits.shape \n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,H,Sk,Sk)).to(device)\n",
    "        causal_mask = causal_mask[:,:,-Sq:,-Sk:] # Trim off, for kv_cache, no-op if kv_cache is None\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(attention_logits.dtype).min\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits,mask_value=None):\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(masked_attention_logits.dtype).min\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(\n",
    "            attention_mask[:,None,None,:] == 0, # Additional dimension for heads\n",
    "            mask_value)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        B,H,_,E = value.shape\n",
    "        B,H,Sq,Sk = attention_weights.shape\n",
    "\n",
    "        attention_output = attention_weights@value\n",
    "        \n",
    "        attention_output = self._merge_head(attention_output) # flatten back to (B,S,E)\n",
    "\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None,kv_cache=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x,kv_cache)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "S = 3\n",
    "E = 32\n",
    "H = 8\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "multi_headed_attn = BaseMultiHeadedAttention(E,H)\n",
    "output = multi_headed_attn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input for kv-cache testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "H = 8\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "multi_headed_attn = BaseMultiHeadedAttention(E,H)\n",
    "\n",
    "output = multi_headed_attn(z,attention_mask=None)\n",
    "cache = KVCache(output.key,output.value) # Construct kv cache\n",
    "\n",
    "# new token\n",
    "_z = torch.rand(B,1,E)\n",
    "\n",
    "# Without KV Caching\n",
    "z_new = torch.concat([z,_z],dim=1)\n",
    "output_new = multi_headed_attn(z_new,attention_mask=None)\n",
    "\n",
    "# With KV Caching\n",
    "output_with_caching = multi_headed_attn(_z,attention_mask=None,kv_cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0735,  0.2351, -0.2685,  0.0621,  0.4552, -0.2058, -0.1648,\n",
       "          -0.0123,  0.1202,  0.1113, -0.2080, -0.2728, -0.0924,  0.0647,\n",
       "          -0.0148, -0.0551]],\n",
       "\n",
       "        [[ 0.2660,  0.3878, -0.1069,  0.0234,  0.4454, -0.0942, -0.1740,\n",
       "          -0.1417,  0.1913,  0.1965, -0.2677, -0.1547, -0.0307,  0.0504,\n",
       "          -0.0111,  0.0865]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0735,  0.2351, -0.2685,  0.0621,  0.4552, -0.2058, -0.1648,\n",
       "          -0.0123,  0.1202,  0.1113, -0.2080, -0.2728, -0.0924,  0.0647,\n",
       "          -0.0148, -0.0551]],\n",
       "\n",
       "        [[ 0.2660,  0.3878, -0.1069,  0.0234,  0.4454, -0.0942, -0.1740,\n",
       "          -0.1417,  0.1913,  0.1965, -0.2677, -0.1547, -0.0307,  0.0504,\n",
       "          -0.0111,  0.0865]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention mask\n",
    "E = 16\n",
    "H = 4\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "multi_headed_attn = BaseMultiHeadedAttention(E,H)\n",
    "output = multi_headed_attn(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a unit test, since the test cases are being repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AttentionTestCase:\n",
    "    def __init__(self,Module,B,S,E,model_kwargs):\n",
    "        self.Module = Module\n",
    "        self.B = B\n",
    "        self.S = S\n",
    "        self.E = E\n",
    "        self.model_kwargs = model_kwargs\n",
    "\n",
    "    def test_forward(self):\n",
    "        z = torch.rand(self.B,self.S,self.E)\n",
    "\n",
    "        module = self.Module(**self.model_kwargs)\n",
    "        output = module(z)\n",
    "        assert output.attention_output.shape == (self.B,self.S,self.E)\n",
    "        print('Forward test passed!')\n",
    "\n",
    "    def test_attention_mask(self):\n",
    "        from transformers import AutoTokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.padding_side = \"left\" # Important!,usually decoder only models require left padding\n",
    "\n",
    "        \n",
    "        texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "        encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "        attention_mask = encoded_input['attention_mask']\n",
    "        B,S = attention_mask.shape\n",
    "\n",
    "        z = torch.rand(B,S,self.E)\n",
    "\n",
    "        module = self.Module(**self.model_kwargs)\n",
    "        output = module(z,attention_mask)\n",
    "        assert output.attention_output.shape == (B,S,self.E)\n",
    "\n",
    "        print('Attention mask test passed!')\n",
    "\n",
    "    def test_kv_cache(self):\n",
    "        z = torch.rand(self.B,self.S,self.E)\n",
    "        module = self.Module(**self.model_kwargs)\n",
    "\n",
    "        output = module(z,attention_mask=None)\n",
    "        cache = KVCache(output.key,output.value) # Construct kv cache\n",
    "\n",
    "        # new token\n",
    "        _z = torch.rand(self.B,1,self.E)\n",
    "\n",
    "        # Without KV Caching\n",
    "        z_new = torch.concat([z,_z],dim=1)\n",
    "        output_new = module(z_new,attention_mask=None)\n",
    "\n",
    "        # With KV Caching\n",
    "        output_with_caching = module(_z,attention_mask=None,kv_cache=cache)\n",
    "\n",
    "        assert (output_new.attention_output[:,-1:,:] - output_with_caching.attention_output).abs().max().item() < 1e-5,\\\n",
    "              \"Attention output does not match: kv-caching\"\n",
    "        \n",
    "        print('KV cache test passed!')\n",
    "        \n",
    "    def run(self):\n",
    "        self.test_forward()\n",
    "        self.test_attention_mask()\n",
    "        self.test_kv_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward test passed!\n",
      "Attention mask test passed!\n",
      "KV cache test passed!\n"
     ]
    }
   ],
   "source": [
    "testing = AttentionTestCase(BaseMultiHeadedAttention,B=2,S=3,E=32,model_kwargs={'embed_dim':32,'num_heads':8})\n",
    "testing.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouped Query Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePreTrainedGroupedQueryAttention(BaseMultiHeadedAttention):\n",
    "    \"\"\"\n",
    "    Modifies *trained* Multi-Headed Attention to Grouped Query Attention as done in https://arxiv.org/pdf/2305.13245v3.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim,num_heads,num_groups):\n",
    "        super().__init__(embed_dim,num_heads)\n",
    "        self.num_groups = num_groups\n",
    "        assert num_heads%num_groups==0, \"num_heads must be divisible by num_groups\"\n",
    "\n",
    "    def _group(self,x):\n",
    "        \"\"\"\n",
    "        Grouping with mean pooling as suggested by https://arxiv.org/pdf/2305.13245v3.pdf\n",
    "\n",
    "        key,value shape: B,H,S,E => B,G,H//G,S,E ===mean pooling===> B,G,S,E\n",
    "        To ensure order is correct, we permute to B,S,E,H then group to B,S,E,G,H//G\n",
    "        Then mean pool B,S,E,G,H//G => B,S,E,G ===permute===> B,G,S,E\n",
    "        Then Interleave repeat to B,H,S,E\n",
    "        \"\"\"\n",
    "\n",
    "        B,H,S,E = x.shape\n",
    "        G = self.num_groups\n",
    "\n",
    "        x = x.permute([0,2,3,1]) # B,S,E,H\n",
    "        x = x.reshape(B,S,E,G,H//G) # B,S,E,G,H//G\n",
    "        x = x.mean(dim=-1) # B,S,E,G\n",
    "        x = x.permute([0,3,1,2]) # B,G,S,E\n",
    "        x = torch.repeat_interleave(x,H//G,dim=1)\n",
    "        return x\n",
    "\n",
    "    def construct_query_key_value(self, x,kv_cache=None):\n",
    "        query,key,value =  super().construct_query_key_value(x)\n",
    "\n",
    "        B,H,S,E = key.shape\n",
    "        G = self.num_groups\n",
    "\n",
    "        key = self._group(key)\n",
    "        value = self._group(value)\n",
    "\n",
    "        return query,key,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward test passed!\n",
      "Attention mask test passed!\n"
     ]
    }
   ],
   "source": [
    "testing = AttentionTestCase(BasePreTrainedGroupedQueryAttention,B=2,S=3,E=32\n",
    "                            ,model_kwargs={'embed_dim':32,'num_groups':2,'num_heads':8})\n",
    "\n",
    "testing.test_forward()\n",
    "testing.test_attention_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGroupedQueryAttention(BaseMultiHeadedAttention):\n",
    "    \"\"\"\n",
    "    Implementation for training form scratch, \n",
    "    directly project to grouped query instead of mean pooling like done in Mistral\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim,num_heads,num_groups):\n",
    "        self.num_groups = num_groups\n",
    "        super().__init__(embed_dim,num_heads)\n",
    "        assert num_heads%num_groups==0, \"num_heads must be divisible by num_groups\"\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        kv_head_embed_dim = self.num_groups * (self.embed_dim//self.num_heads)\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "        self.key_proj = nn.Linear(self.embed_dim,kv_head_embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,kv_head_embed_dim)\n",
    "\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "\n",
    "    def construct_query_key_value(self, x,kv_cache=None):\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "     \n",
    "        query = self._split_head(query,self.num_heads)\n",
    "        key = self._split_head(key,self.num_groups)\n",
    "        value = self._split_head(value,self.num_groups)\n",
    "\n",
    "        key = torch.repeat_interleave(key,self.num_heads//self.num_groups,dim=1)\n",
    "        value = torch.repeat_interleave(value,self.num_heads//self.num_groups,dim=1)\n",
    "\n",
    "        return query,key,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask test passed!\n",
      "Forward test passed!\n",
      "\n",
      "Attention mask test passed!\n",
      "Forward test passed!\n"
     ]
    }
   ],
   "source": [
    "# Same as multi-query\n",
    "testing = AttentionTestCase(BaseGroupedQueryAttention,B=2,S=3,E=32\n",
    "                            ,model_kwargs={'embed_dim':32,'num_groups':1,'num_heads':8})\n",
    "\n",
    "testing.test_attention_mask()\n",
    "testing.test_forward()\n",
    "print()\n",
    "# Same as grouped query\n",
    "testing = AttentionTestCase(BaseGroupedQueryAttention,B=2,S=3,E=32\n",
    "                            ,model_kwargs={'embed_dim':32,'num_groups':4,'num_heads':8})\n",
    "\n",
    "testing.test_attention_mask()\n",
    "testing.test_forward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
