{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention: baseline for ref\n",
    "Functionality wise most of the code remains same. Hacky way to reuse self-attention code is by reshaping the q,k,v to shape `(batch*num_heads, S, E//num_heads)`. \n",
    "\n",
    "We will implement from scratch with seperate dimension for heads for better flexibility later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from utils.data_containers import BaseOutput\n",
    "\n",
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x,kv_cache):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        if kv_cache is not None:\n",
    "            key = torch.concat([kv_cache.key,\n",
    "                    key],dim=1)\n",
    "            value = torch.concat([kv_cache.value,\n",
    "                      value],dim=1)\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,Sq,Sk = attention_logits.shape \n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device)\n",
    "        causal_mask = causal_mask[:,-Sq:,-Sk:] # Trim off, for kv_cache, no-op if kv_cache is None\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = -torch.inf\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits):\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, -torch.inf)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None,kv_cache=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x,kv_cache=kv_cache)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "S = 8\n",
    "E = 32\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "self_attn = BaseSelfAttention(E)\n",
    "output = self_attn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
