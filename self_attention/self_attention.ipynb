{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch numpy scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention: Bits, use it inspect and visualize line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first setup a small dummy input for testing and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "S = 8\n",
    "E = 16\n",
    "\n",
    "z = torch.rand(B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct query, key and value from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_proj = nn.Linear(E,E)\n",
    "key_proj = nn.Linear(E,E)\n",
    "value_proj = nn.Linear(E,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_proj(z)\n",
    "key = key_proj(z)\n",
    "value = value_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 16]), torch.Size([2, 8, 16]), torch.Size([2, 8, 16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_t = key.transpose(1,2)\n",
    "key_t.shape # Q.K' : [B,S,E] @ [B,E,S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_attention_logits = (query@key_t) # Q.K' : [B,S,E] @ [B,E,S] => [B,S,S]\n",
    "unscaled_attention_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale our attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,E = query.shape\n",
    "scale = math.sqrt(E)\n",
    "attention_logits = unscaled_attention_logits/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.inf\n",
    "causal_mask = torch.tril(torch.ones(B,S,S)) # lower trianglular matrix with 1s\n",
    "masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) # replace upper triangular with -inf or a very large negative number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1272,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0857,  0.0085,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0985,  0.0315,  0.1365,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.2192,  0.1274,  0.1895, -0.0568,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.2630,  0.2194,  0.3101,  0.0008,  0.0192,    -inf,    -inf,    -inf],\n",
       "        [ 0.1405,  0.0805,  0.1534, -0.0623, -0.0633,  0.0253,    -inf,    -inf],\n",
       "        [ 0.0920,  0.0191,  0.1043, -0.1186, -0.1003, -0.0371,  0.0185,    -inf],\n",
       "        [ 0.1840,  0.1545,  0.2070, -0.0107,  0.0201,  0.0729,  0.1638,  0.1078]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attention_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attention_weights = torch.softmax(masked_attention_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5193, 0.4807, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3363, 0.3145, 0.3493, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2746, 0.2505, 0.2666, 0.2084, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2194, 0.2100, 0.2299, 0.1688, 0.1719, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1826, 0.1719, 0.1849, 0.1490, 0.1489, 0.1627, 0.0000, 0.0000],\n",
       "         [0.1566, 0.1456, 0.1586, 0.1269, 0.1292, 0.1376, 0.1455, 0.0000],\n",
       "         [0.1339, 0.1300, 0.1370, 0.1102, 0.1137, 0.1198, 0.1312, 0.1241]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4531, 0.5469, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2794, 0.3415, 0.3791, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2106, 0.2464, 0.2905, 0.2524, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1897, 0.2116, 0.2106, 0.1865, 0.2015, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1405, 0.1721, 0.1802, 0.1680, 0.1824, 0.1568, 0.0000, 0.0000],\n",
       "         [0.1155, 0.1428, 0.1598, 0.1482, 0.1505, 0.1352, 0.1480, 0.0000],\n",
       "         [0.1099, 0.1292, 0.1332, 0.1222, 0.1295, 0.1235, 0.1251, 0.1274]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_weights # Observe lower triangular! Causal attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = causal_attention_weights@value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about padding tokens?\n",
    "* In the base implementation we used self-attention on every-tokens, including padding tokens if present!\n",
    "* Now we extend the base implementation so that attention is ignored for tokens like pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at a sample tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ch20b103sivasankar/Works/LLM-Components/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # Important!,usually decoder only models require left padding\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id # pad token id for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,     2,     2,     2,     2,     2,     1, 13264,  5509])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'][0] # Observe left padding!, 2 is the pad token\n",
    "# Refer: https://huggingface.co/docs/transformers/llm_tutorial#wrong-padding-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['attention_mask'][0] # Observe attention mask is zero for pad tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle attention mask in the attention logits\n",
    "Also look at https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.1245, 0.6143,\n",
       "          0.4404],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.7113, 0.8817,\n",
       "          0.9789],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.0140, 0.7529,\n",
       "          0.9597],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.2815, 0.8038,\n",
       "          0.3988],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.0323, 0.8905,\n",
       "          0.1319],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.2209, 0.4885,\n",
       "          0.5274],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.0901, 0.3648,\n",
       "          0.5164],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.3190, 0.0750,\n",
       "          0.5282],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.5381, 0.0880,\n",
       "          0.0667]],\n",
       "\n",
       "        [[  -inf,   -inf,   -inf,   -inf, 0.9039, 0.7427, 0.1361, 0.9384,\n",
       "          0.6286],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.1962, 0.6371, 0.3871, 0.4674,\n",
       "          0.4492],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.9051, 0.3013, 0.8864, 0.9562,\n",
       "          0.1307],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.8836, 0.2835, 0.3447, 0.0782,\n",
       "          0.4481],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.1661, 0.7689, 0.2864, 0.8786,\n",
       "          0.1567],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.1694, 0.1037, 0.7376, 0.8059,\n",
       "          0.9047],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.6202, 0.4620, 0.4170, 0.9883,\n",
       "          0.1794],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.8237, 0.3267, 0.6612, 0.0204,\n",
       "          0.4307],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.8336, 0.1643, 0.6774, 0.4142,\n",
       "          0.1073]],\n",
       "\n",
       "        [[0.6455, 0.5313, 0.1306, 0.9100, 0.8527, 0.3363, 0.7270, 0.8341,\n",
       "          0.1270],\n",
       "         [0.9368, 0.5185, 0.6414, 0.2878, 0.4667, 0.8279, 0.1267, 0.8740,\n",
       "          0.5757],\n",
       "         [0.4168, 0.7106, 0.5404, 0.3213, 0.6883, 0.7725, 0.0104, 0.2261,\n",
       "          0.3147],\n",
       "         [0.0580, 0.7429, 0.9635, 0.0195, 0.2203, 0.0193, 0.1344, 0.6068,\n",
       "          0.9585],\n",
       "         [0.0429, 0.9603, 0.3014, 0.4669, 0.3549, 0.7228, 0.2135, 0.4213,\n",
       "          0.2067],\n",
       "         [0.2885, 0.0066, 0.3313, 0.0399, 0.9266, 0.6755, 0.5842, 0.4594,\n",
       "          0.1984],\n",
       "         [0.1544, 0.8408, 0.2291, 0.4749, 0.5436, 0.3367, 0.5316, 0.4637,\n",
       "          0.6239],\n",
       "         [0.4728, 0.8935, 0.6479, 0.7491, 0.2100, 0.8537, 0.9190, 0.8352,\n",
       "          0.5196],\n",
       "         [0.5869, 0.6690, 0.0945, 0.8633, 0.5173, 0.5850, 0.9029, 0.1838,\n",
       "          0.6292]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_logits = torch.rand(B, S, S) # dummy for demo\n",
    "attention_logits = attention_logits.masked_fill_(attention_mask[:,None,:] == 0, -torch.inf)\n",
    "attention_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the code bits together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOutput:\n",
    "    def __init__(self,**kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "\n",
    "    Not Implemented:\n",
    "    * KV cacheing\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,S,S = attention_logits.shape\n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,S,S)).to(device)\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = -torch.inf\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits):\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, -torch.inf)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 16\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output = self_atten(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-V caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith KV caching:\\nInput: (B,1,E), kv_cache\\nOutput: (B,1,E)\\n\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\\\n",
    "\n",
    "With KV caching:\n",
    "Input: (B,1,E), kv_cache\n",
    "Output: (B,1,E)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Dummy input for testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "\n",
    "z_old = torch.rand(B,S,E)\n",
    "z = torch.rand(B,1,E)\n",
    "z_new = torch.concat([z_old,z],dim=1)\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output_old = self_atten(z_old,attention_mask=None)\n",
    "output_new = self_atten(z_new,attention_mask=None)\n",
    "\n",
    "print(z.shape)\n",
    "print(z_old.shape)\n",
    "print(z_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self,key,value,**kwargs):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store previous K,V in the cache\n",
    "cache = KVCache(output_old.key,\n",
    "                output_old.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct K,V from cache\n",
    "\n",
    "new_key = self_atten.key_proj(z)\n",
    "new_value = self_atten.value_proj(z)\n",
    "query = self_atten.query_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = torch.concat([cache.key,\n",
    "                    new_key],dim=1)\n",
    "value = torch.concat([cache.value,\n",
    "                      new_value],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [2, 1, 5] doesn't match the broadcast shape [2, 5, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m unmasked_attention_logits \u001b[38;5;241m=\u001b[39m self_atten\u001b[38;5;241m.\u001b[39mcalculate_unmasked_attention_logits(query,key)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Apply causal masking\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m masked_attention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mself_atten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43munmasked_attention_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate attention weights\u001b[39;00m\n\u001b[1;32m     10\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m self_atten\u001b[38;5;241m.\u001b[39mcalculate_attention_weights(masked_attention_logits)\n",
      "Cell \u001b[0;32mIn[26], line 49\u001b[0m, in \u001b[0;36mBaseSelfAttention.apply_causal_mask\u001b[0;34m(self, attention_logits, mask_value)\u001b[0m\n\u001b[1;32m     46\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# replace upper triangular with -inf or a very large negative number, causal masking!\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m masked_attention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mattention_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m masked_attention_logits\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [2, 1, 5] doesn't match the broadcast shape [2, 5, 5]"
     ]
    }
   ],
   "source": [
    "# remaining operations remains the same\n",
    "\n",
    "# Calculate logits\n",
    "unmasked_attention_logits = self_atten.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "# Apply causal masking\n",
    "masked_attention_logits = self_atten.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "# Calculate attention weights\n",
    "attention_weights = self_atten.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "# And finally, Calculate the final output\n",
    "attention_output = self_atten.calculate_final_output(attention_weights,value)\n",
    "\n",
    "output_with_caching = BaseOutput(attention_output=attention_output,\n",
    "                    attention_weights=attention_weights,\n",
    "                    key=key,\n",
    "                    value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, causal mask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n",
      "torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# let's replicate it\n",
    "\n",
    "attention_logits = self_atten.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "B,Sq,Sk = attention_logits.shape # <---------Change 1\n",
    "\n",
    "device = attention_logits.device\n",
    "causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device) # <------------Change 2\n",
    "\n",
    "print(causal_mask.shape)\n",
    "causal_mask = causal_mask[:,-Sq:,-Sk:] # <------------Change 3\n",
    "print(causal_mask.shape)\n",
    "\n",
    "if mask_value is None:\n",
    "    mask_value = -torch.inf\n",
    "\n",
    "# replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue from that\n",
    "\n",
    "# Calculate attention weights\n",
    "attention_weights = self_atten.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "# And finally, Calculate the final output\n",
    "attention_output = self_atten.calculate_final_output(attention_weights,value)\n",
    "\n",
    "output_with_caching = BaseOutput(attention_output=attention_output,\n",
    "                    attention_weights=attention_weights,\n",
    "                    key=key,\n",
    "                    value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0684, -0.0154, -0.0904, -0.2118,  0.2850,  0.2744,  0.3194,\n",
       "           0.4253,  0.1098,  0.1277, -0.3167,  0.3301, -0.3709, -0.0795,\n",
       "           0.1013,  0.1084]],\n",
       "\n",
       "        [[ 0.1241, -0.0053, -0.1390, -0.1733,  0.2012,  0.3066,  0.2656,\n",
       "           0.5026,  0.1187,  0.1604, -0.3240,  0.3251, -0.3033, -0.1132,\n",
       "           0.0834,  0.1254]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0684, -0.0154, -0.0904, -0.2118,  0.2850,  0.2744,  0.3194,\n",
       "           0.4253,  0.1098,  0.1277, -0.3167,  0.3301, -0.3709, -0.0795,\n",
       "           0.1013,  0.1084]],\n",
       "\n",
       "        [[ 0.1241, -0.0053, -0.1390, -0.1733,  0.2012,  0.3066,  0.2656,\n",
       "           0.5026,  0.1187,  0.1604, -0.3240,  0.3251, -0.3033, -0.1132,\n",
       "           0.0834,  0.1254]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge it with the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x,kv_cache):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        if kv_cache is not None:\n",
    "            key = torch.concat([kv_cache.key,\n",
    "                    key],dim=1)\n",
    "            value = torch.concat([kv_cache.value,\n",
    "                      value],dim=1)\n",
    "\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,Sq,Sk = attention_logits.shape \n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device)\n",
    "        causal_mask = causal_mask[:,-Sq:,-Sk:] # Trim off, for kv_cache, no-op if kv_cache is None\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = -torch.inf\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits):\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, -torch.inf)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None,kv_cache=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x,kv_cache=kv_cache)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self,key,value,**kwargs):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n",
    "\n",
    "\n",
    "class BaseOutput:\n",
    "    def __init__(self,**kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 16\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output = self_atten(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input for kv-cache testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "self_atten = BaseSelfAttention(E)\n",
    "\n",
    "output = self_atten(z,attention_mask=None)\n",
    "cache = KVCache(output.key,output.value) # Construct kv cache\n",
    "\n",
    "# new token\n",
    "_z = torch.rand(B,1,E)\n",
    "\n",
    "# Without KV Caching\n",
    "z_new = torch.concat([z,_z],dim=1)\n",
    "output_new = self_atten(z_new,attention_mask=None)\n",
    "\n",
    "# With KV Caching\n",
    "output_with_caching = self_atten(_z,attention_mask=None,kv_cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0030,  0.2412,  0.3421,  0.2261,  0.1717, -0.2394,  0.0092,  0.1346,\n",
       "          0.4956, -0.2014, -0.0524, -0.2519, -0.1964, -0.0980,  0.4925, -0.0917],\n",
       "        [ 0.0101,  0.2290,  0.3033,  0.2238,  0.1581, -0.2247,  0.0239,  0.1448,\n",
       "          0.4721, -0.1742, -0.0281, -0.2448, -0.1844, -0.0562,  0.4522, -0.0282]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0030,  0.2412,  0.3421,  0.2261,  0.1717, -0.2394,  0.0092,\n",
       "           0.1346,  0.4956, -0.2014, -0.0524, -0.2519, -0.1964, -0.0980,\n",
       "           0.4925, -0.0917]],\n",
       "\n",
       "        [[ 0.0101,  0.2290,  0.3033,  0.2238,  0.1581, -0.2247,  0.0239,\n",
       "           0.1448,  0.4721, -0.1742, -0.0281, -0.2448, -0.1844, -0.0562,\n",
       "           0.4522, -0.0282]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.4901e-08,  1.4901e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          -1.4901e-08,  1.1176e-08,  2.9802e-08,  2.9802e-08, -2.9802e-08,\n",
       "          -7.4506e-09,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00]],\n",
       "\n",
       "        [[-7.4506e-09,  0.0000e+00,  0.0000e+00, -1.4901e-08,  0.0000e+00,\n",
       "           0.0000e+00, -3.7253e-09,  0.0000e+00,  2.9802e-08, -2.9802e-08,\n",
       "           0.0000e+00,  1.4901e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:] - output_with_caching.attention_output\n",
    "# Just to verify, some numerical errors are expected only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
