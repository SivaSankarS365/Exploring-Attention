{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch numpy scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention: Bits, use it inspect and visualize line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first setup a small dummy input for testing and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "S = 8\n",
    "E = 16\n",
    "\n",
    "z = torch.rand(B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct query, key and value from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_proj = nn.Linear(E,E)\n",
    "key_proj = nn.Linear(E,E)\n",
    "value_proj = nn.Linear(E,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_proj(z)\n",
    "key = key_proj(z)\n",
    "value = value_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 16]), torch.Size([2, 8, 16]), torch.Size([2, 8, 16]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_t = key.transpose(1,2)\n",
    "key_t.shape # Q.K' : [B,S,E] @ [B,E,S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_attention_logits = (query@key_t) # Q.K' : [B,S,E] @ [B,E,S] => [B,S,S]\n",
    "unscaled_attention_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale our attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,E = query.shape\n",
    "scale = math.sqrt(E)\n",
    "attention_logits = unscaled_attention_logits/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.inf\n",
    "causal_mask = torch.tril(torch.ones(B,S,S)) # lower trianglular matrix with 1s\n",
    "masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) # replace upper triangular with -inf or a very large negative number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0864,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1881, 0.1825,   -inf,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1470, 0.0837, 0.0767,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.0975, 0.0718, 0.0193, 0.0382,   -inf,   -inf,   -inf,   -inf],\n",
       "        [0.1623, 0.1190, 0.0545, 0.0852, 0.1564,   -inf,   -inf,   -inf],\n",
       "        [0.1410, 0.1174, 0.1031, 0.0749, 0.1418, 0.0471,   -inf,   -inf],\n",
       "        [0.2412, 0.2278, 0.1846, 0.1734, 0.1910, 0.1089, 0.1918,   -inf],\n",
       "        [0.1133, 0.0457, 0.0184, 0.0343, 0.0964, 0.0371, 0.0608, 0.0761]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attention_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attention_weights = torch.softmax(masked_attention_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5014, 0.4986, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3483, 0.3270, 0.3247, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2603, 0.2537, 0.2407, 0.2453, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2094, 0.2005, 0.1880, 0.1939, 0.2082, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1728, 0.1688, 0.1664, 0.1618, 0.1729, 0.1573, 0.0000, 0.0000],\n",
       "         [0.1505, 0.1485, 0.1422, 0.1406, 0.1431, 0.1318, 0.1432, 0.0000],\n",
       "         [0.1318, 0.1231, 0.1198, 0.1217, 0.1295, 0.1221, 0.1250, 0.1269]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5246, 0.4754, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3351, 0.3284, 0.3365, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2513, 0.2301, 0.2582, 0.2604, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1985, 0.1871, 0.2092, 0.1945, 0.2107, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1678, 0.1593, 0.1757, 0.1681, 0.1734, 0.1557, 0.0000, 0.0000],\n",
       "         [0.1454, 0.1354, 0.1488, 0.1464, 0.1509, 0.1337, 0.1394, 0.0000],\n",
       "         [0.1289, 0.1210, 0.1286, 0.1284, 0.1284, 0.1182, 0.1177, 0.1286]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_weights # Observe lower triangular! Causal attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = causal_attention_weights@value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about padding tokens?\n",
    "* In the base implementation we used self-attention on every-tokens, including padding tokens if present!\n",
    "* Now we extend the base implementation so that attention is ignored for tokens like pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at a sample tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ch20b103sivasankar/Works/LLM-Components/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # Important!,usually decoder only models require left padding\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id # pad token id for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,     2,     2,     2,     2,     2,     1, 13264,  5509])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'][0] # Observe left padding!, 2 is the pad token\n",
    "# Refer: https://huggingface.co/docs/transformers/llm_tutorial#wrong-padding-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['attention_mask'][0] # Observe attention mask is zero for pad tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle attention mask in the attention logits\n",
    "Also look at https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.8878, 0.6595,\n",
       "          0.8409],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.2274, 0.0165,\n",
       "          0.2792],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.7158, 0.7744,\n",
       "          0.5028],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.6947, 0.7066,\n",
       "          0.5426],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.6911, 0.4306,\n",
       "          0.5193],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.2244, 0.3229,\n",
       "          0.6242],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.8498, 0.7114,\n",
       "          0.7502],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.3790, 0.9751,\n",
       "          0.5675],\n",
       "         [  -inf,   -inf,   -inf,   -inf,   -inf,   -inf, 0.6106, 0.8722,\n",
       "          0.1585]],\n",
       "\n",
       "        [[  -inf,   -inf,   -inf,   -inf, 0.1535, 0.9526, 0.8391, 0.5240,\n",
       "          0.9733],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.5833, 0.0900, 0.5204, 0.2608,\n",
       "          0.3748],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.9246, 0.6689, 0.9091, 0.4451,\n",
       "          0.6386],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.0323, 0.3335, 0.3705, 0.9737,\n",
       "          0.2291],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.4043, 0.8764, 0.6170, 0.0370,\n",
       "          0.4788],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.3287, 0.4120, 0.1011, 0.9760,\n",
       "          0.1424],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.0788, 0.8645, 0.3566, 0.7704,\n",
       "          0.5509],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.5827, 0.4816, 0.8516, 0.6371,\n",
       "          0.1192],\n",
       "         [  -inf,   -inf,   -inf,   -inf, 0.6186, 0.0311, 0.2837, 0.7430,\n",
       "          0.1334]],\n",
       "\n",
       "        [[0.5745, 0.2009, 0.9537, 0.4587, 0.6519, 0.2159, 0.2652, 0.4792,\n",
       "          0.8128],\n",
       "         [0.6032, 0.8943, 0.4853, 0.9744, 0.0603, 0.5415, 0.6342, 0.9745,\n",
       "          0.4032],\n",
       "         [0.8402, 0.4370, 0.5284, 0.3159, 0.1530, 0.8291, 0.6710, 0.7974,\n",
       "          0.1530],\n",
       "         [0.5078, 0.6300, 0.9008, 0.7196, 0.1775, 0.8528, 0.8706, 0.3046,\n",
       "          0.6966],\n",
       "         [0.5817, 0.1495, 0.0526, 0.8265, 0.6641, 0.1309, 0.6940, 0.4320,\n",
       "          0.4333],\n",
       "         [0.5361, 0.6799, 0.3874, 0.6538, 0.6760, 0.9118, 0.4693, 0.5282,\n",
       "          0.9701],\n",
       "         [0.7411, 0.6644, 0.3226, 0.1848, 0.0944, 0.6276, 0.0537, 0.2824,\n",
       "          0.3518],\n",
       "         [0.9544, 0.2080, 0.5287, 0.0352, 0.0535, 0.2452, 0.5080, 0.5821,\n",
       "          0.0775],\n",
       "         [0.8813, 0.0220, 0.9885, 0.5236, 0.1857, 0.8923, 0.0627, 0.9635,\n",
       "          0.6638]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_logits = torch.rand(B, S, S) # dummy for demo\n",
    "attention_logits = attention_logits.masked_fill_(attention_mask[:,None,:] == 0, -torch.inf)\n",
    "attention_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the code bits together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOutput:\n",
    "    def __init__(self,**kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "\n",
    "    Not Implemented:\n",
    "    * KV cacheing\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,S,S = attention_logits.shape\n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,S,S)).to(device)\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = -torch.inf\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits):\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, -torch.inf)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 16\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output = self_atten(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-V caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith KV caching:\\nInput: (B,1,E), kv_cache\\nOutput: (B,1,E)\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\\\n",
    "\n",
    "With KV caching:\n",
    "Input: (B,1,E), kv_cache\n",
    "Output: (B,1,E)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Dummy input for testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "\n",
    "z_old = torch.rand(B,S,E)\n",
    "z = torch.rand(B,1,E)\n",
    "z_new = torch.concat([z_old,z],dim=1)\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output_old = self_atten(z_old,attention_mask=None)\n",
    "output_new = self_atten(z_new,attention_mask=None)\n",
    "\n",
    "print(z.shape)\n",
    "print(z_old.shape)\n",
    "print(z_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self,key,value,**kwargs):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store previous K,V in the cache\n",
    "cache = KVCache(output_old.key,\n",
    "                output_old.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct K,V from cache\n",
    "\n",
    "new_key = self_atten.key_proj(z)\n",
    "new_value = self_atten.value_proj(z)\n",
    "query = self_atten.query_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = torch.concat([cache.key,\n",
    "                    new_key],dim=1)\n",
    "value = torch.concat([cache.value,\n",
    "                      new_value],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 16])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [2, 1, 5] doesn't match the broadcast shape [2, 5, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m unmasked_attention_logits \u001b[38;5;241m=\u001b[39m self_atten\u001b[38;5;241m.\u001b[39mcalculate_unmasked_attention_logits(query,key)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Apply causal masking\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m masked_attention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mself_atten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43munmasked_attention_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate attention weights\u001b[39;00m\n\u001b[1;32m     10\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m self_atten\u001b[38;5;241m.\u001b[39mcalculate_attention_weights(masked_attention_logits)\n",
      "Cell \u001b[0;32mIn[24], line 49\u001b[0m, in \u001b[0;36mBaseSelfAttention.apply_causal_mask\u001b[0;34m(self, attention_logits, mask_value)\u001b[0m\n\u001b[1;32m     46\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mtorch\u001b[38;5;241m.\u001b[39minf\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# replace upper triangular with -inf or a very large negative number, causal masking!\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m masked_attention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mattention_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m masked_attention_logits\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [2, 1, 5] doesn't match the broadcast shape [2, 5, 5]"
     ]
    }
   ],
   "source": [
    "# remaining operations remains the same\n",
    "\n",
    "# Calculate logits\n",
    "unmasked_attention_logits = self_atten.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "# Apply causal masking\n",
    "masked_attention_logits = self_atten.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "# Calculate attention weights\n",
    "attention_weights = self_atten.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "# And finally, Calculate the final output\n",
    "attention_output = self_atten.calculate_final_output(attention_weights,value)\n",
    "\n",
    "output_with_caching = BaseOutput(attention_output=attention_output,\n",
    "                    attention_weights=attention_weights,\n",
    "                    key=key,\n",
    "                    value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, causal mask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n",
      "torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# let's replicate it\n",
    "\n",
    "attention_logits = self_atten.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "B,Sq,Sk = attention_logits.shape # <---------Change 1\n",
    "\n",
    "device = attention_logits.device\n",
    "causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device) # <------------Change 2\n",
    "\n",
    "print(causal_mask.shape)\n",
    "causal_mask = causal_mask[:,-Sq:,-Sk:] # <------------Change 3\n",
    "print(causal_mask.shape)\n",
    "\n",
    "if mask_value is None:\n",
    "    mask_value = -torch.inf\n",
    "\n",
    "# replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue from that\n",
    "\n",
    "# Calculate attention weights\n",
    "attention_weights = self_atten.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "# And finally, Calculate the final output\n",
    "attention_output = self_atten.calculate_final_output(attention_weights,value)\n",
    "\n",
    "output_with_caching = BaseOutput(attention_output=attention_output,\n",
    "                    attention_weights=attention_weights,\n",
    "                    key=key,\n",
    "                    value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0490, -0.1678,  0.1818, -0.1398,  0.0972,  0.2274, -0.1216,\n",
       "          -0.0604, -0.0800,  0.1481,  0.2079, -0.1148,  0.0112, -0.0232,\n",
       "           0.2457, -0.1065]],\n",
       "\n",
       "        [[-0.0963, -0.2137,  0.1641, -0.1136,  0.0437,  0.2227, -0.0582,\n",
       "          -0.0935, -0.0542,  0.2538,  0.1606, -0.0999,  0.0909,  0.0347,\n",
       "           0.1008, -0.1780]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0490, -0.1678,  0.1818, -0.1398,  0.0972,  0.2274, -0.1216,\n",
       "          -0.0604, -0.0800,  0.1481,  0.2079, -0.1148,  0.0112, -0.0232,\n",
       "           0.2457, -0.1065]],\n",
       "\n",
       "        [[-0.0963, -0.2137,  0.1641, -0.1136,  0.0437,  0.2227, -0.0582,\n",
       "          -0.0935, -0.0542,  0.2538,  0.1606, -0.0999,  0.0909,  0.0347,\n",
       "           0.1008, -0.1780]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge it with the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x,kv_cache):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        if kv_cache is not None:\n",
    "            key = torch.concat([kv_cache.key,\n",
    "                    key],dim=1)\n",
    "            value = torch.concat([kv_cache.value,\n",
    "                      value],dim=1)\n",
    "\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,Sq,Sk = attention_logits.shape \n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device)\n",
    "        causal_mask = causal_mask[:,-Sq:,-Sk:] # Trim off, for kv_cache, no-op if kv_cache is None\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = -torch.inf\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits):\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, -torch.inf)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None,kv_cache=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x,kv_cache=kv_cache)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self,key,value,**kwargs):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n",
    "\n",
    "\n",
    "class BaseOutput:\n",
    "    def __init__(self,**kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 16\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output = self_atten(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input for kv-cache testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "self_atten = BaseSelfAttention(E)\n",
    "\n",
    "output = self_atten(z,attention_mask=None)\n",
    "cache = KVCache(output.key,output.value) # Construct kv cache\n",
    "\n",
    "# new token\n",
    "_z = torch.rand(B,1,E)\n",
    "\n",
    "# Without KV Caching\n",
    "z_new = torch.concat([z,_z],dim=1)\n",
    "output_new = self_atten(z_new,attention_mask=None)\n",
    "\n",
    "# With KV Caching\n",
    "output_with_caching = self_atten(_z,attention_mask=None,kv_cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1764,  0.1235,  0.1896,  0.1131, -0.1038, -0.2366, -0.5676, -0.0113,\n",
       "          0.2021,  0.4325, -0.1776, -0.4810,  0.0634, -0.0687, -0.3003, -0.3440],\n",
       "        [-0.1879,  0.1474,  0.1990,  0.1931, -0.0349, -0.1754, -0.4873, -0.0399,\n",
       "          0.2542,  0.4419, -0.2270, -0.4308, -0.0149, -0.1404, -0.3198, -0.3030]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1764,  0.1235,  0.1896,  0.1131, -0.1038, -0.2366, -0.5676,\n",
       "          -0.0113,  0.2021,  0.4325, -0.1776, -0.4810,  0.0634, -0.0687,\n",
       "          -0.3003, -0.3440]],\n",
       "\n",
       "        [[-0.1879,  0.1474,  0.1990,  0.1931, -0.0349, -0.1754, -0.4873,\n",
       "          -0.0399,  0.2542,  0.4419, -0.2270, -0.4308, -0.0149, -0.1404,\n",
       "          -0.3198, -0.3030]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.9802e-08,  7.4506e-09,  1.4901e-08,  1.4901e-08, -1.4901e-08,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  2.9802e-08,  2.9802e-08,\n",
       "          -1.4901e-08,  0.0000e+00,  0.0000e+00, -1.4901e-08,  0.0000e+00,\n",
       "          -5.9605e-08]],\n",
       "\n",
       "        [[ 0.0000e+00, -1.4901e-08,  1.4901e-08,  0.0000e+00,  1.4901e-08,\n",
       "          -1.4901e-08,  2.9802e-08,  0.0000e+00, -2.9802e-08,  0.0000e+00,\n",
       "           2.9802e-08, -2.9802e-08,  2.9802e-08,  2.9802e-08,  0.0000e+00,\n",
       "           2.9802e-08]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:] - output_with_caching.attention_output\n",
    "# Just to verify, some numerical errors are expected only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to multiple-heads | Multi-Headed Attention\n",
    "Functionality wise most of the code remains same. Hacky way to reuse self-attention code is by reshaping the q,k,v to shape `(batch*num_heads, S, E//num_heads)`. \n",
    "\n",
    "We will implement from scratch with seperate dimension for heads for better flexibility later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Q,K,V into heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "S = 8\n",
    "E = 32\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "self_attn = BaseSelfAttention(E)\n",
    "output = self_attn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the self-attention implementation\n",
    "\n",
    "query,key,value = self_attn.construct_query_key_value(z,kv_cache=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "\n",
    "\n",
    "B,S,E = query.shape\n",
    "assert E%num_heads==0, \"embed_dim must be divisible by num_heads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.view(B,num_heads,S,E//num_heads)\n",
    "key = key.view(B,num_heads,S,E//num_heads)\n",
    "value = value.view(B,num_heads,S,E//num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 8, 8])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape # now (B,num_heads,S,E//num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this, the operations are same as the original self-attention.\n",
    "# however,Some changes are required to incoperate an additional head dimension\n",
    "# After all operations, we merge back the heads before final output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = torch.rand(B,num_heads,S,E//num_heads) # dummy for testing\n",
    "\n",
    "attention_output = attention_output.view(B,S,E) # go back to (B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend self-attention to multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMultiHeadedAttention(BaseSelfAttention):\n",
    "    \"\"\"\n",
    "    Extends self attention to multi-headed attention\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim,num_heads):\n",
    "        super().__init__(embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "    \n",
    "    def create_heads(self,query,key,value):\n",
    "        B,S,E = query.shape\n",
    "        query = query.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        key = key.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        value = value.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        return query,key,value\n",
    "    \n",
    "        \n",
    "    def construct_query_key_value(self,x,kv_cache):\n",
    "        query,key,value = super().construct_query_key_value(x,kv_cache)\n",
    "        return self.create_heads(query,key,value)\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,H, S,E] @ [B,H,E,S]\n",
    "        key_t = key.transpose(2,3) # Transpose to [B,H,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,H,Sq,Sk = attention_logits.shape \n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,H,Sk,Sk)).to(device)\n",
    "        causal_mask = causal_mask[:,:,-Sq:,-Sk:] # Trim off, for kv_cache, no-op if kv_cache is None\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = -torch.inf\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits):\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(\n",
    "            attention_mask[:,None,None,:] == 0, # Additional dimension for heads\n",
    "            -torch.inf)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "\n",
    "        attention_output = attention_output.view(B,S,E) # flatten back to (B,S,E)\n",
    "\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "S = 8\n",
    "E = 32\n",
    "H = 4\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "self_attn = BaseMultiHeadedAttention(E,H)\n",
    "output = self_attn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 32])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.attention_output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
