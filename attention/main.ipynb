{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch numpy scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# from imports import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention: Bits, use it inspect and visualize line by line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first setup a small dummy input for testing and visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "S = 8\n",
    "E = 16\n",
    "\n",
    "z = torch.rand(B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct query, key and value from the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_proj = nn.Linear(E,E)\n",
    "key_proj = nn.Linear(E,E)\n",
    "value_proj = nn.Linear(E,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query_proj(z)\n",
    "key = key_proj(z)\n",
    "value = value_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 8, 16]), torch.Size([2, 8, 16]), torch.Size([2, 8, 16]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape, key.shape, value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_t = key.transpose(1,2)\n",
    "key_t.shape # Q.K' : [B,S,E] @ [B,E,S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_attention_logits = (query@key_t) # Q.K' : [B,S,E] @ [B,E,S] => [B,S,S]\n",
    "unscaled_attention_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale our attention logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,E = query.shape\n",
    "scale = math.sqrt(E)\n",
    "attention_logits = unscaled_attention_logits/scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 8])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_value = -torch.inf\n",
    "causal_mask = torch.tril(torch.ones(B,S,S)) # lower trianglular matrix with 1s\n",
    "masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) # replace upper triangular with -inf or a very large negative number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1104,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0592,  0.1330,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.0450,  0.1087,  0.0230,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1133,  0.1776, -0.0067,  0.0827,    -inf,    -inf,    -inf,    -inf],\n",
       "        [ 0.1373,  0.2221,  0.0218,  0.0863,  0.0739,    -inf,    -inf,    -inf],\n",
       "        [ 0.1535,  0.2603,  0.0694,  0.1739,  0.1543, -0.0314,    -inf,    -inf],\n",
       "        [ 0.0565,  0.1109, -0.0508,  0.0451,  0.0285, -0.1157, -0.0383,    -inf],\n",
       "        [ 0.1122,  0.2090,  0.0232,  0.0874,  0.1077, -0.0731,  0.0905,  0.0062]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_attention_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_attention_weights = torch.softmax(masked_attention_logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4816, 0.5184, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.3285, 0.3501, 0.3214, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2549, 0.2718, 0.2261, 0.2472, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2054, 0.2236, 0.1830, 0.1952, 0.1928, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1699, 0.1891, 0.1562, 0.1734, 0.1701, 0.1412, 0.0000, 0.0000],\n",
       "         [0.1500, 0.1584, 0.1347, 0.1483, 0.1459, 0.1263, 0.1364, 0.0000],\n",
       "         [0.1299, 0.1431, 0.1189, 0.1267, 0.1293, 0.1080, 0.1271, 0.1169]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4380, 0.5620, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2925, 0.3659, 0.3416, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2197, 0.2767, 0.2443, 0.2593, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1777, 0.2216, 0.2190, 0.1891, 0.1927, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1471, 0.1805, 0.1676, 0.1604, 0.1547, 0.1897, 0.0000, 0.0000],\n",
       "         [0.1220, 0.1558, 0.1354, 0.1333, 0.1332, 0.1623, 0.1580, 0.0000],\n",
       "         [0.1031, 0.1340, 0.1231, 0.1123, 0.1143, 0.1509, 0.1386, 0.1237]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attention_weights # Observe lower triangular! Causal attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = causal_attention_weights@value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about padding tokens?\n",
    "* In the base implementation we used self-attention on every-tokens, including padding tokens if present!\n",
    "* Now we extend the base implementation so that attention is ignored for tokens like pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at a sample tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ch20b103sivasankar/Works/LLM-Components/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\" # Important!,usually decoder only models require left padding\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id # pad token id for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    2,     2,     2,     2,     2,     2,     1, 13264,  5509])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids'][0] # Observe left padding!, 2 is the pad token\n",
    "# Refer: https://huggingface.co/docs/transformers/llm_tutorial#wrong-padding-side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['attention_mask'][0] # Observe attention mask is zero for pad tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle attention mask in the attention logits\n",
    "Also look at https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  9.5617e-01,  3.2460e-02,  1.0315e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  6.8204e-01,  9.1030e-01,  9.7699e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  8.2957e-01,  7.8932e-01,  9.3458e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  1.9415e-01,  1.4572e-02,  1.9848e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  2.4337e-01,  3.5080e-01,  2.0121e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  1.0501e-01,  9.4756e-01,  4.5054e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  3.7606e-01,  4.1200e-01,  5.6446e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  5.1225e-01,  1.2906e-01,  6.9631e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "          -3.4028e+38,  1.8280e-01,  4.1090e-01,  4.2432e-01]],\n",
       "\n",
       "        [[-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  5.8195e-01,\n",
       "           5.9499e-01,  1.8996e-01,  8.7734e-01,  6.9488e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  6.8163e-02,\n",
       "           6.4410e-01,  8.8955e-01,  6.4601e-01,  3.0283e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  5.3961e-01,\n",
       "           5.7863e-01,  8.8444e-01,  3.7591e-01,  2.6822e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  2.5191e-01,\n",
       "           9.9246e-01,  4.9715e-01,  9.6473e-01,  5.3433e-03],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  7.7972e-01,\n",
       "           5.5923e-01,  5.7488e-01,  2.3086e-01,  7.0036e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  3.0975e-01,\n",
       "           8.9606e-01,  8.6058e-01,  1.9723e-01,  9.9584e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  5.4814e-01,\n",
       "           6.4948e-01,  9.0101e-01,  9.5171e-01,  5.3481e-02],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  5.3004e-01,\n",
       "           1.7470e-01,  7.8860e-01,  8.5441e-01,  5.0030e-01],\n",
       "         [-3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,  1.0926e-01,\n",
       "           9.8888e-01,  6.8802e-02,  9.1175e-01,  6.0380e-01]],\n",
       "\n",
       "        [[ 5.5860e-02,  2.0901e-01,  4.2206e-01,  8.4783e-01,  9.9634e-01,\n",
       "           6.7885e-01,  7.4750e-01,  6.6516e-01,  5.6517e-01],\n",
       "         [ 5.5241e-01,  2.4061e-01,  1.4504e-01,  4.0418e-01,  7.4408e-01,\n",
       "           2.7562e-01,  2.7220e-01,  5.3850e-01,  6.6287e-01],\n",
       "         [ 4.9611e-01,  3.2698e-01,  1.9760e-01,  4.9234e-01,  9.5300e-01,\n",
       "           7.7353e-01,  1.2908e-01,  5.3764e-01,  1.8419e-02],\n",
       "         [ 1.3157e-01,  4.9803e-01,  1.2829e-01,  7.3922e-02,  4.1439e-01,\n",
       "           6.5670e-01,  2.8450e-01,  8.8167e-01,  3.1733e-01],\n",
       "         [ 5.2647e-01,  8.6665e-01,  8.9156e-01,  2.4315e-01,  6.7181e-01,\n",
       "           1.2486e-01,  5.6912e-01,  5.9822e-01,  8.4326e-01],\n",
       "         [ 6.6153e-01,  5.1280e-01,  9.0396e-01,  4.8681e-01,  2.4449e-01,\n",
       "           3.5823e-01,  3.6447e-01,  4.5043e-01,  6.8256e-01],\n",
       "         [ 1.5921e-01,  9.1979e-01,  6.7969e-01,  4.7595e-01,  6.5448e-03,\n",
       "           5.2724e-01,  7.6049e-01,  4.8168e-01,  7.6141e-01],\n",
       "         [ 2.0420e-01,  7.4813e-02,  4.8751e-01,  2.8844e-01,  3.2243e-02,\n",
       "           5.9675e-01,  1.1143e-01,  8.2845e-01,  3.4536e-01],\n",
       "         [ 1.9255e-01,  1.2651e-01,  9.4325e-01,  4.1485e-01,  8.8715e-01,\n",
       "           8.0821e-01,  6.4485e-01,  3.3784e-01,  7.4369e-01]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_logits = torch.rand(B, S, S) # dummy for demo\n",
    "attention_logits = attention_logits.masked_fill_(attention_mask[:,None,:] == 0, torch.finfo(attention_logits.dtype).min)\n",
    "attention_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting the code bits together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseOutput:\n",
    "    def __init__(self,**kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "\n",
    "    Not Implemented:\n",
    "    * KV cacheing\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,S,S = attention_logits.shape\n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,S,S)).to(device)\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(attention_logits.dtype).min\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits,mask_value=None):\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(masked_attention_logits.dtype).min\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, mask_value)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 16\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output = self_atten(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-V caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith KV caching:\\nInput: (B,1,E), kv_cache\\nOutput: (B,1,E)\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\\\n",
    "\n",
    "With KV caching:\n",
    "Input: (B,1,E), kv_cache\n",
    "Output: (B,1,E)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 16])\n",
      "torch.Size([2, 4, 16])\n",
      "torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Dummy input for testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "\n",
    "z_old = torch.rand(B,S,E)\n",
    "z = torch.rand(B,1,E)\n",
    "z_new = torch.concat([z_old,z],dim=1)\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output_old = self_atten(z_old,attention_mask=None)\n",
    "output_new = self_atten(z_new,attention_mask=None)\n",
    "\n",
    "print(z.shape)\n",
    "print(z_old.shape)\n",
    "print(z_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCache:\n",
    "    def __init__(self,key,value,**kwargs):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store previous K,V in the cache\n",
    "cache = KVCache(output_old.key,\n",
    "                output_old.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct K,V from cache\n",
    "\n",
    "new_key = self_atten.key_proj(z)\n",
    "new_value = self_atten.value_proj(z)\n",
    "query = self_atten.query_proj(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = torch.concat([cache.key,\n",
    "                    new_key],dim=1)\n",
    "value = torch.concat([cache.value,\n",
    "                      new_value],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 16])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 16])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [2, 1, 5] doesn't match the broadcast shape [2, 5, 5]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m unmasked_attention_logits \u001b[38;5;241m=\u001b[39m self_atten\u001b[38;5;241m.\u001b[39mcalculate_unmasked_attention_logits(query,key)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Apply causal masking\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m masked_attention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mself_atten\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_causal_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43munmasked_attention_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate attention weights\u001b[39;00m\n\u001b[1;32m     10\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m self_atten\u001b[38;5;241m.\u001b[39mcalculate_attention_weights(masked_attention_logits)\n",
      "Cell \u001b[0;32mIn[28], line 49\u001b[0m, in \u001b[0;36mBaseSelfAttention.apply_causal_mask\u001b[0;34m(self, attention_logits, mask_value)\u001b[0m\n\u001b[1;32m     46\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attention_logits\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# replace upper triangular with -inf or a very large negative number, causal masking!\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m masked_attention_logits \u001b[38;5;241m=\u001b[39m \u001b[43mattention_logits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m masked_attention_logits\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [2, 1, 5] doesn't match the broadcast shape [2, 5, 5]"
     ]
    }
   ],
   "source": [
    "# remaining operations remains the same\n",
    "\n",
    "# Calculate logits\n",
    "unmasked_attention_logits = self_atten.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "# Apply causal masking\n",
    "masked_attention_logits = self_atten.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "# Calculate attention weights\n",
    "attention_weights = self_atten.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "# And finally, Calculate the final output\n",
    "attention_output = self_atten.calculate_final_output(attention_weights,value)\n",
    "\n",
    "output_with_caching = BaseOutput(attention_output=attention_output,\n",
    "                    attention_weights=attention_weights,\n",
    "                    key=key,\n",
    "                    value=value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, causal mask!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 5])\n",
      "torch.Size([2, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# let's replicate it\n",
    "\n",
    "attention_logits = self_atten.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "B,Sq,Sk = attention_logits.shape # <---------Change 1\n",
    "\n",
    "device = attention_logits.device\n",
    "causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device) # <------------Change 2\n",
    "\n",
    "print(causal_mask.shape)\n",
    "causal_mask = causal_mask[:,-Sq:,-Sk:] # <------------Change 3\n",
    "print(causal_mask.shape)\n",
    "\n",
    "if mask_value is None:\n",
    "    mask_value = -torch.inf\n",
    "\n",
    "# replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue from that\n",
    "\n",
    "# Calculate attention weights\n",
    "attention_weights = self_atten.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "# And finally, Calculate the final output\n",
    "attention_output = self_atten.calculate_final_output(attention_weights,value)\n",
    "\n",
    "output_with_caching = BaseOutput(attention_output=attention_output,\n",
    "                    attention_weights=attention_weights,\n",
    "                    key=key,\n",
    "                    value=value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0332,  0.1659, -0.0441,  0.4275, -0.4797, -0.0975, -0.0763,\n",
       "          -0.1674, -0.3293, -0.1751, -0.1188, -0.1235, -0.2550,  0.3202,\n",
       "           0.0678,  0.0646]],\n",
       "\n",
       "        [[ 0.0223,  0.1878, -0.0288,  0.4413, -0.5024, -0.1070, -0.0152,\n",
       "          -0.1167, -0.3277, -0.1006, -0.1549, -0.1229, -0.3062,  0.2441,\n",
       "           0.0094,  0.0572]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0332,  0.1659, -0.0441,  0.4275, -0.4797, -0.0975, -0.0763,\n",
       "          -0.1674, -0.3293, -0.1751, -0.1188, -0.1235, -0.2550,  0.3202,\n",
       "           0.0678,  0.0646]],\n",
       "\n",
       "        [[ 0.0223,  0.1878, -0.0288,  0.4413, -0.5024, -0.1070, -0.0152,\n",
       "          -0.1167, -0.3277, -0.1006, -0.1549, -0.1229, -0.3062,  0.2441,\n",
       "           0.0094,  0.0572]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! matches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge it with the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def construct_query_key_value(self,x,kv_cache):\n",
    "        # Construct Q, K, V\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "        if kv_cache is not None:\n",
    "            key = torch.concat([kv_cache.key,\n",
    "                    key],dim=1)\n",
    "            value = torch.concat([kv_cache.value,\n",
    "                      value],dim=1)\n",
    "\n",
    "        return query, key, value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,S,E] @ [B,E,S]\n",
    "        key_t = key.transpose(1,2) # Transpose to [B,E,S] by exchanging dim 1 and 2\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,Sq,Sk = attention_logits.shape \n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,Sk,Sk)).to(device)\n",
    "        causal_mask = causal_mask[:,-Sq:,-Sk:] # Trim off, for kv_cache, no-op if kv_cache is None\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(attention_logits.dtype).min\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits,mask_value=None):\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(masked_attention_logits.dtype).min\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(attention_mask[:,None,:] == 0, mask_value)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        attention_output = attention_weights@value\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None,kv_cache=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x,kv_cache=kv_cache)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output\n",
    "    \n",
    "\n",
    "\n",
    "class KVCache:\n",
    "    def __init__(self,key,value,**kwargs):\n",
    "        self.key = key\n",
    "        self.value = value\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n",
    "\n",
    "\n",
    "class BaseOutput:\n",
    "    def __init__(self,**kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self,k,v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "E = 16\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "self_atten = BaseSelfAttention(E)\n",
    "output = self_atten(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input for kv-cache testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "self_atten = BaseSelfAttention(E)\n",
    "\n",
    "output = self_atten(z,attention_mask=None)\n",
    "cache = KVCache(output.key,output.value) # Construct kv cache\n",
    "\n",
    "# new token\n",
    "_z = torch.rand(B,1,E)\n",
    "\n",
    "# Without KV Caching\n",
    "z_new = torch.concat([z,_z],dim=1)\n",
    "output_new = self_atten(z_new,attention_mask=None)\n",
    "\n",
    "# With KV Caching\n",
    "output_with_caching = self_atten(_z,attention_mask=None,kv_cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2702, -0.0455, -0.0794,  0.0350, -0.1609,  0.1403,  0.1773, -0.4252,\n",
       "          0.0762, -0.2927,  0.3254, -0.1199,  0.0784, -0.3484, -0.4058,  0.0879],\n",
       "        [ 0.2043, -0.0176, -0.0551, -0.0361, -0.1820,  0.0297,  0.1552, -0.3377,\n",
       "          0.1246, -0.3020,  0.2412, -0.1095,  0.1516, -0.2699, -0.3367,  0.1868]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2702, -0.0455, -0.0794,  0.0350, -0.1609,  0.1403,  0.1773,\n",
       "          -0.4252,  0.0762, -0.2927,  0.3254, -0.1199,  0.0784, -0.3484,\n",
       "          -0.4058,  0.0879]],\n",
       "\n",
       "        [[ 0.2043, -0.0176, -0.0551, -0.0361, -0.1820,  0.0297,  0.1552,\n",
       "          -0.3377,  0.1246, -0.3020,  0.2412, -0.1095,  0.1516, -0.2699,\n",
       "          -0.3367,  0.1868]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.4901e-08,\n",
       "           2.9802e-08,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00, -1.4901e-08,  2.9802e-08,  0.0000e+00,\n",
       "          -7.4506e-09]],\n",
       "\n",
       "        [[-1.4901e-08, -2.2352e-08,  0.0000e+00, -7.4506e-09,  1.4901e-08,\n",
       "          -3.7253e-09,  0.0000e+00,  0.0000e+00,  2.2352e-08, -2.9802e-08,\n",
       "           2.9802e-08, -7.4506e-09,  0.0000e+00,  0.0000e+00,  2.9802e-08,\n",
       "           0.0000e+00]]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:] - output_with_caching.attention_output\n",
    "# Just to verify, some numerical errors are expected only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend to multiple-heads | Multi-Headed Attention\n",
    "Functionality wise most of the code remains same. Hacky way to reuse self-attention code is by reshaping the q,k,v to shape `(batch*num_heads, S, E//num_heads)`. \n",
    "\n",
    "We will implement from scratch with seperate dimension for heads for better flexibility later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Q,K,V into heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "S = 8\n",
    "E = 32\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "self_attn = BaseSelfAttention(E)\n",
    "output = self_attn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the self-attention implementation\n",
    "\n",
    "query,key,value = self_attn.construct_query_key_value(z,kv_cache=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 4\n",
    "\n",
    "\n",
    "B,S,E = query.shape\n",
    "assert E%num_heads==0, \"embed_dim must be divisible by num_heads\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = query.view(B,num_heads,S,E//num_heads)\n",
    "key = key.view(B,num_heads,S,E//num_heads)\n",
    "value = value.view(B,num_heads,S,E//num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 8, 8])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query.shape # now (B,num_heads,S,E//num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After this, the operations are same as the original self-attention.\n",
    "# however,Some changes are required to incoperate an additional head dimension\n",
    "# After all operations, we merge back the heads before final output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_output = torch.rand(B,num_heads,S,E//num_heads) # dummy for testing\n",
    "\n",
    "attention_output = attention_output.view(B,S,E) # go back to (B,S,E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend self-attention to multi-headed attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.arange(6*32).view(2,3,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "B,S,E = z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Minimal Base Self Attention\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.init_qkvo_proj()\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.key_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "    def _split_head(self,x,num_heads):\n",
    "        B,S,E = x.shape\n",
    "        assert E%num_heads==0, \"embed_dim must be divisible by num_heads\"\n",
    "        x = x.view(B,S,E//num_heads,num_heads) # B,S,E,H\n",
    "        x = x.permute([0,3,1,2]) # B,H, S, E\n",
    "        return x\n",
    "    \n",
    "    def _merge_head(self,x):\n",
    "        B,H,S,E = x.shape\n",
    "        x = x.permute([0,2,1,3]) # B,S,H,E\n",
    "        x = x.reshape(B,S,H*E) # B,S,H*E\n",
    "        return x\n",
    "\n",
    "\n",
    "    def create_heads(self,query,key,value):\n",
    "        B,S,E = query.shape\n",
    "        assert E%self.num_heads==0, \"embed_dim must be divisible by num_heads\"\n",
    "        # query = query.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        # key = key.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        # value = value.view(B,self.num_heads,S,E//self.num_heads)\n",
    "        query = self._split_head(query,self.num_heads)\n",
    "        key = self._split_head(key,self.num_heads)\n",
    "        value = self._split_head(value,self.num_heads)\n",
    "        return query,key,value\n",
    "\n",
    "    def construct_query_key_value(self,x,kv_cache):\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "\n",
    "        query,key,value = self.create_heads(query,key,value)\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            key = torch.concat([kv_cache.key,\n",
    "                    key],dim=2) # B,H,S,E. dim becomes 2\n",
    "            value = torch.concat([kv_cache.value,\n",
    "                      value],dim=2)\n",
    "        \n",
    "        return query,key,value\n",
    "    \n",
    "    def calculate_unmasked_attention_logits(self,query,key):\n",
    "        # Q.K' : [B,H,S,E] @ [B,H,E,S]\n",
    "        key_t = key.transpose(2,3) # Transpose to [B,H,E,S] by exchanging dim 2 and 3\n",
    "        \n",
    "        # scaling factor\n",
    "        scale = math.sqrt(self.embed_dim)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = (query@key_t)/scale\n",
    "        return unmasked_attention_logits\n",
    "    \n",
    "    def apply_causal_mask(self,attention_logits,mask_value=None):\n",
    "        # lower trianglular matrix with 1s\n",
    "        B,H,Sq,Sk = attention_logits.shape \n",
    "\n",
    "        device = attention_logits.device\n",
    "        causal_mask = torch.tril(torch.ones(B,H,Sk,Sk)).to(device)\n",
    "        causal_mask = causal_mask[:,:,-Sq:,-Sk:] # Trim off, for kv_cache, no-op if kv_cache is None\n",
    "\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(attention_logits.dtype).min\n",
    "\n",
    "        # replace upper triangular with -inf or a very large negative number, causal masking!\n",
    "        masked_attention_logits = attention_logits.masked_fill_(causal_mask == 0, mask_value) \n",
    "        return masked_attention_logits\n",
    "    \n",
    "    def apply_attention_mask(self,attention_mask,masked_attention_logits,mask_value=None):\n",
    "        if mask_value is None:\n",
    "            mask_value = torch.finfo(masked_attention_logits.dtype).min\n",
    "        masked_attention_logits = masked_attention_logits.masked_fill_(\n",
    "            attention_mask[:,None,None,:] == 0, # Additional dimension for heads\n",
    "            mask_value)\n",
    "        return masked_attention_logits\n",
    "    \n",
    "    \n",
    "    def calculate_attention_weights(self,masked_attention_logits):\n",
    "        attention_weights = torch.softmax(masked_attention_logits, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def calculate_final_output(self,attention_weights,value):\n",
    "        B,H,_,E = value.shape\n",
    "        B,H,Sq,Sk = attention_weights.shape\n",
    "\n",
    "        attention_output = attention_weights@value\n",
    "        \n",
    "        attention_output = self._merge_head(attention_output) # flatten back to (B,S,E)\n",
    "\n",
    "        final_output = self.output_proj(attention_output)\n",
    "        return final_output\n",
    "    \n",
    "    def forward(self, x,attention_mask=None,kv_cache=None):\n",
    "        # Construct Q, K, V\n",
    "        query, key, value = self.construct_query_key_value(x,kv_cache=kv_cache)\n",
    "\n",
    "        # Calculate logits\n",
    "        unmasked_attention_logits = self.calculate_unmasked_attention_logits(query,key)\n",
    "\n",
    "        # Apply causal masking\n",
    "        masked_attention_logits = self.apply_causal_mask(unmasked_attention_logits)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # Apply attention mask\n",
    "            masked_attention_logits = self.apply_attention_mask(attention_mask,masked_attention_logits)\n",
    "\n",
    "        # Calculate attention weights\n",
    "        attention_weights = self.calculate_attention_weights(masked_attention_logits)\n",
    "\n",
    "        # And finally, Calculate the final output\n",
    "        attention_output = self.calculate_final_output(attention_weights,value)\n",
    "\n",
    "        output = BaseOutput(attention_output=attention_output,\n",
    "                            attention_weights=attention_weights,\n",
    "                            key=key,\n",
    "                            value=value)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 4\n",
    "S = 3\n",
    "E = 32\n",
    "H = 8\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "multi_headed_attn = BaseMultiHeadedAttention(E,H)\n",
    "output = multi_headed_attn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.attention_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy input for kv-cache testing\n",
    "B = 2\n",
    "S = 4\n",
    "E = 16\n",
    "H = 8\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "multi_headed_attn = BaseMultiHeadedAttention(E,H)\n",
    "\n",
    "output = multi_headed_attn(z,attention_mask=None)\n",
    "cache = KVCache(output.key,output.value) # Construct kv cache\n",
    "\n",
    "# new token\n",
    "_z = torch.rand(B,1,E)\n",
    "\n",
    "# Without KV Caching\n",
    "z_new = torch.concat([z,_z],dim=1)\n",
    "output_new = multi_headed_attn(z_new,attention_mask=None)\n",
    "\n",
    "# With KV Caching\n",
    "output_with_caching = multi_headed_attn(_z,attention_mask=None,kv_cache=cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0607, -0.0614, -0.2348, -0.4526, -0.0983,  0.3194,  0.0798,\n",
       "           0.0944,  0.2969, -0.3214,  0.0311,  0.3004, -0.1775, -0.5601,\n",
       "           0.1528,  0.1161]],\n",
       "\n",
       "        [[ 0.0357, -0.0586, -0.2431, -0.4274, -0.1170,  0.3940,  0.0592,\n",
       "           0.1295,  0.4053, -0.3705,  0.0928,  0.3760, -0.1673, -0.5725,\n",
       "           0.0668,  0.0803]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_new.attention_output[:,-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0607, -0.0614, -0.2348, -0.4526, -0.0983,  0.3194,  0.0798,\n",
       "           0.0944,  0.2969, -0.3214,  0.0311,  0.3004, -0.1775, -0.5601,\n",
       "           0.1528,  0.1161]],\n",
       "\n",
       "        [[ 0.0357, -0.0586, -0.2431, -0.4274, -0.1170,  0.3940,  0.0592,\n",
       "           0.1295,  0.4053, -0.3705,  0.0928,  0.3760, -0.1673, -0.5725,\n",
       "           0.0668,  0.0803]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_with_caching.attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test attention mask\n",
    "E = 16\n",
    "H = 4\n",
    "\n",
    "texts = [\"Something random\",\"A bit longer text\",\"something even longer than the two before!\"]\n",
    "encoded_input = tokenizer(texts, return_tensors='pt',padding=True)\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "B,S = attention_mask.shape\n",
    "\n",
    "z = torch.rand(B,S,E)\n",
    "\n",
    "\n",
    "\n",
    "multi_headed_attn = BaseMultiHeadedAttention(E,H)\n",
    "output = multi_headed_attn(z,attention_mask)\n",
    "assert output.attention_output.shape == (B,S,E)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
