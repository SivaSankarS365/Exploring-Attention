{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import *\n",
    "from attention import BaseMultiHeadedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePreTrainedGroupedQueryAttention(BaseMultiHeadedAttention):\n",
    "    \"\"\"\n",
    "    Modifies *trained* Multi-Headed Attention to Grouped Query Attention as done in https://arxiv.org/pdf/2305.13245v3.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim,num_heads,num_groups):\n",
    "        super().__init__(embed_dim,num_heads)\n",
    "        self.num_groups = num_groups\n",
    "        assert num_heads%num_groups==0, \"num_heads must be divisible by num_groups\"\n",
    "\n",
    "    def _group(self,x):\n",
    "        \"\"\"\n",
    "        Grouping with mean pooling as suggested by https://arxiv.org/pdf/2305.13245v3.pdf\n",
    "\n",
    "        key,value shape: B,H,S,E => B,G,H//G,S,E ===mean pooling===> B,G,S,E\n",
    "        To ensure order is correct, we permute to B,S,E,H then group to B,S,E,G,H//G\n",
    "        Then mean pool B,S,E,G,H//G => B,S,E,G ===permute===> B,G,S,E\n",
    "        Then Interleave repeat to B,H,S,E\n",
    "        \"\"\"\n",
    "\n",
    "        B,H,S,E = x.shape\n",
    "        G = self.num_groups\n",
    "\n",
    "        x = x.permute([0,2,3,1]) # B,S,E,H\n",
    "        x = x.reshape(B,S,E,G,H//G) # B,S,E,G,H//G\n",
    "        x = x.mean(dim=-1) # B,S,E,G\n",
    "        x = x.permute([0,3,1,2]) # B,G,S,E\n",
    "        x = torch.repeat_interleave(x,H//G,dim=1)\n",
    "        return x\n",
    "\n",
    "    def construct_query_key_value(self, x):\n",
    "        query,key,value =  super().construct_query_key_value(x)\n",
    "\n",
    "        B,H,S,E = key.shape\n",
    "        G = self.num_groups\n",
    "\n",
    "        key = self._group(key)\n",
    "        value = self._group(value)\n",
    "\n",
    "        return query,key,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward test passed!\n",
      "Attention mask test passed!\n",
      "KV cache test passed!\n"
     ]
    }
   ],
   "source": [
    "from attention.test import AttentionTestCase\n",
    "\n",
    "testing = AttentionTestCase(BasePreTrainedGroupedQueryAttention,B=2,S=3,E=32\n",
    "                            ,model_kwargs={'embed_dim':32,'num_groups':2,'num_heads':8}).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query: B,H,S,E\n",
    "# Key: B,G,S,E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseGroupedQueryAttention(BaseMultiHeadedAttention):\n",
    "    \"\"\"\n",
    "    Implementation for training form scratch, \n",
    "    directly project to grouped query instead of mean pooling like done in Mistral\n",
    "    \"\"\"\n",
    "    def __init__(self,embed_dim,num_heads,num_groups):\n",
    "        self.num_groups = num_groups\n",
    "        super().__init__(embed_dim,num_heads)\n",
    "        assert num_heads%num_groups==0, \"num_heads must be divisible by num_groups\"\n",
    "\n",
    "    def init_qkvo_proj(self):\n",
    "        kv_head_embed_dim = self.num_groups * (self.embed_dim//self.num_heads)\n",
    "        self.query_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "        self.key_proj = nn.Linear(self.embed_dim,kv_head_embed_dim)\n",
    "        self.value_proj = nn.Linear(self.embed_dim,kv_head_embed_dim)\n",
    "\n",
    "        self.output_proj = nn.Linear(self.embed_dim,self.embed_dim)\n",
    "\n",
    "\n",
    "    def construct_query_key_value(self, x):\n",
    "        query = self.query_proj(x)\n",
    "        key = self.key_proj(x)\n",
    "        value = self.value_proj(x)\n",
    "     \n",
    "        query = self._split_head(query,self.num_heads)\n",
    "        key = self._split_head(key,self.num_groups)\n",
    "        value = self._split_head(value,self.num_groups)\n",
    "\n",
    "        key = torch.repeat_interleave(key,self.num_heads//self.num_groups,dim=1)\n",
    "        value = torch.repeat_interleave(value,self.num_heads//self.num_groups,dim=1)\n",
    "\n",
    "        return query,key,value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward test passed!\n",
      "Attention mask test passed!\n",
      "KV cache test passed!\n",
      "\n",
      "Forward test passed!\n",
      "Attention mask test passed!\n",
      "KV cache test passed!\n"
     ]
    }
   ],
   "source": [
    "from attention.test import AttentionTestCase\n",
    "\n",
    "# Same as multi-query\n",
    "testing = AttentionTestCase(BaseGroupedQueryAttention,B=2,S=3,E=32\n",
    "                            ,model_kwargs={'embed_dim':32,'num_groups':1,'num_heads':8}).run()\n",
    "\n",
    "print()\n",
    "# Same as grouped query\n",
    "testing = AttentionTestCase(BaseGroupedQueryAttention,B=2,S=3,E=32\n",
    "                            ,model_kwargs={'embed_dim':32,'num_groups':4,'num_heads':8}).run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
